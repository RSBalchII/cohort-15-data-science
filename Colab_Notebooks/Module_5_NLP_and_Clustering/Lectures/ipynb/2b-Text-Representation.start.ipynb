{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lEO9w9HDXbAa"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","pd.options.display.max_columns = 100\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZyXbs2dsq80"},"outputs":[],"source":["%%capture\n","!python -m textblob.download_corpora\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ul__IwLAXxJ4"},"outputs":[],"source":["sentence_1 = 'Jen is a good student.'\n","sentence_2 = 'Jen is also a great guitarist.'\n","sentence_3 = 'Good students can sometimes be good guitarists'\n"]},{"cell_type":"markdown","metadata":{"id":"SAQ_Cn1J05Pk"},"source":["# Data Cleaning\n","We want to singularize guitarists and students."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HATpf-dN091J"},"outputs":[],"source":["sentence_3_tb = TextBlob(sentence_3) # Make a textblob so that we can singularize the word\n","sentence_3_singular = [x.singularize() for x in sentence_3_tb.words] # Singularize each word in the text\n","sentence_3_clean = ' '.join(sentence_3_singular) # Join it together into a single string\n","sentence_3_clean\n"]},{"cell_type":"markdown","metadata":{"id":"yuFf00oinAiU"},"source":["## Bag of Words Using CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pfxrLYnXxsA"},"outputs":[],"source":["# Perform the count transformation\n","vectorizer = CountVectorizer(stop_words='english')\n","bow_vec = vectorizer.fit_transform([sentence_1, sentence_2, sentence_3_clean])\n","bow_vec\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1UGeAWVl-xe"},"outputs":[],"source":["bow_vec.toarray()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GqvVpq_mpbD"},"outputs":[],"source":["# Print out results in a data frame\n","sent_df = pd.DataFrame(bow_vec.toarray(), columns = vectorizer.get_feature_names_out())\n","sent_df\n"]},{"cell_type":"markdown","metadata":{"id":"M-FL1eL1j1zm"},"source":["### Your Turn\n","1. Write 4 sentences of your choice.\n","2. Run the `CountVectorizer` on your sentences.\n","3. Print the results in a data frame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2pxtQbGj1Wu"},"outputs":[],"source":["# Solution 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfqzM2t1kM5x"},"outputs":[],"source":["# Solution 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdR9K3SwkN8S"},"outputs":[],"source":["# Solution 3\n"]},{"cell_type":"markdown","metadata":{"id":"hv0Xa9IHnF7T"},"source":["## TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDR3hA_1nE02"},"outputs":[],"source":["# Perform the TF-IDF transformation - Option 1 (TfidfVectorizer)\n","tf_idf_vec = TfidfVectorizer(stop_words = 'english')\n","tf_idf_jen = tf_idf_vec.fit_transform([sentence_1, sentence_2, sentence_3_clean])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiSRAYrqJGlh"},"outputs":[],"source":["print(sentence_1)\n","print(sentence_2)\n","print(sentence_3_clean)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"699yjSoNocXh"},"outputs":[],"source":["# Print out results in a dataframe\n","tf_df = pd.DataFrame(tf_idf_jen.toarray(), columns = tf_idf_vec.get_feature_names_out())\n","tf_df.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8v7YRCIczv-"},"outputs":[],"source":["tf_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41fBQBzgjjv9"},"outputs":[],"source":["# Perform the TF-IDF transformation - Option 2 (CountVectorizer + TfidfTransformer - better for large datasets)\n","tf_idf_tran = TfidfTransformer()\n","tf_idf_jen = tf_idf_tran.fit_transform(bow_vec)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGEShHQUjsdx"},"outputs":[],"source":["# Print out results in a dataframe\n","tf_df = pd.DataFrame(tf_idf_jen.toarray(), columns = vectorizer.get_feature_names_out())\n","tf_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RytiNKSYDQ1n"},"outputs":[],"source":["# Get a data frame with the TF-IDF values sorted for document 0\n","df = pd.DataFrame(tf_idf_jen[0].T.todense(), index=tf_idf_vec.get_feature_names_out(), columns=[\"TF-IDF\"])\n","df = df.sort_values('TF-IDF', ascending=False)\n","df\n"]},{"cell_type":"code","source":["tf_df.transpose()[0].sort_values(ascending = False)"],"metadata":{"id":"9M7d64cmHh7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXYwykW8kXp9"},"source":["### Your Turn\n","1. Use the `TfidfTransformer` to transform the bag of words matrix you created above to TF-IDF.\n","2. Print out the results in a data frame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Db325TZRk9LW"},"outputs":[],"source":["# Solution 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYToGBF5k-We"},"outputs":[],"source":["# Solution 2\n"]},{"cell_type":"markdown","metadata":{"id":"xDuCEa4Z-mwb"},"source":["# Another Example - Using Wikipedia API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZKrDDzTE32G"},"outputs":[],"source":["%%capture output\n","#install Wikipedia API\n","!pip3 install wikipedia-api\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCyuTKCq-mLN"},"outputs":[],"source":["import wikipediaapi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZ8mxZvMEeDC"},"outputs":[],"source":["# Pull out the popcorn page from wikipedia - https://en.wikipedia.org/wiki/Popcorn\n","topic = 'popcorn'\n","wikip = wikipediaapi.Wikipedia(user_agent = 'foobar')\n","page_ex = wikip.page(topic)\n","wiki_text = page_ex.text\n","wiki_text\n"]},{"cell_type":"markdown","source":["### Clean the text - version 1\n","\n","Using string replace.\n","\n"],"metadata":{"id":"84WagQgIexwB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eWVRMUPJvsw"},"outputs":[],"source":["# Replace newline chars with spaces before doing any processing. Strip the ' and \"s\" from possessives\n","wiki_text_clean = (\n","    wiki_text\n","    .replace(\"\\n\",\" \")\n","    .replace(\"'s\",'')\n","    .replace(\"'\",'')\n",")\n","wiki_text_clean\n"]},{"cell_type":"markdown","source":["### Clean the text - version 2\n","\n","Using a for..loop and string replace.\n","\n","\n"],"metadata":{"id":"IAwddEGlmFo5"}},{"cell_type":"code","source":["wiki_text_clean = wiki_text.lower()\n","for c in [\"\\n\", \"'s\", \"'\", \"  \"]:\n","  wiki_text_clean = wiki_text_clean.replace(c,\" \")\n","wiki_text_clean"],"metadata":{"id":"RMUpsMwWWdgY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Clean the text - version 3\n","\n","Using a regular expression.\n"],"metadata":{"id":"7Xa4VcRuIXqO"}},{"cell_type":"code","source":["import re\n","\n","pat = re.compile(r\"(\\n|'s|'| )+\")\n","wiki_text_clean = re.sub(pat,' ', wiki_text.lower())\n","wiki_text_clean\n"],"metadata":{"id":"G-y0b9eqZKM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDIJiWY1Kwd3"},"outputs":[],"source":["# Break up single string into separate sentences\n","wiki_blob = TextBlob(wiki_text_clean)\n","len(wiki_blob.sentences)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jj7xKSFGLPoU"},"outputs":[],"source":["# Only look at first 5 sentences\n","my_sentences = wiki_blob.sentences[0:5]\n","my_sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUh1z8OiQyDa"},"outputs":[],"source":["# Convert text blob sentences to strings\n","my_sentences_str = [ str(x) for x in my_sentences ]\n","my_sentences_str\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUILAz5BKGB1"},"outputs":[],"source":["# Perform the TF-IDF Vectorization\n","tf_idf_vec = TfidfVectorizer(stop_words = 'english')\n","tf_idf_pop = tf_idf_vec.fit_transform(my_sentences_str)\n","tf_idf_pop.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytxNUAds1KeF"},"outputs":[],"source":["tf_idf_pop.transpose().shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFr_I2y605iF"},"outputs":[],"source":["tf_idf_vec.get_feature_names_out()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxdvcVwFRkut"},"outputs":[],"source":["# Print out results in a dataframe\n","tf_df = pd.DataFrame(tf_idf_pop.toarray(), columns = tf_idf_vec.get_feature_names_out())\n","tf_df.transpose()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZIzZwxuRERt"},"outputs":[],"source":["# Get a data frame with the TF-IDF values sorted for document 0\n","df = pd.DataFrame(tf_idf_pop[0].T.todense(), index=tf_idf_vec.get_feature_names_out(), columns=[\"TF-IDF\"])\n","df = df.sort_values('TF-IDF', ascending=False)\n","df[:5]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qiVEFduJyAxr"},"outputs":[],"source":["tf_df.T[[0]].sort_values([0], ascending=False)[:5]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P7bM8U0vTAQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}