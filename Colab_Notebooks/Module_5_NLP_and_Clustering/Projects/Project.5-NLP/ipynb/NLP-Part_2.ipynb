{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the same person from step 1), use the Wikipedia API to access the whole content of that person's Wikipedia page.\n",
    "- The goal of part 2) is to produce the capability to:\n",
    "  1. For that Wikipedia page determine the sentiment of the entire page\n",
    "  1. Print out the Wikipedia article\n",
    "  1. Collect the Wikipedia pages from the 10 nearest neighbors in Step 1)\n",
    "  1. Determine the nearness ranking of these 10 to your main subject based on their entire Wikipedia page\n",
    "  1. Compare the nearest ranking from Step 1) with the Wikipedia page nearness ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install wikipedia\n",
    "\n",
    "import wikipedia\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(person_name):\n",
    "    try:\n",
    "        page = wikipedia.page(person_name)\n",
    "        return page.content\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    singularized_words = [Word(word).singularize() for word in words]\n",
    "    return ' '.join(singularized_words)\n",
    "\n",
    "def analyze_wikipedia_content(person_name, nearest_neighbors):\n",
    "    # Fetch Wikipedia content for the main person\n",
    "    main_content = get_wikipedia_content(person_name)\n",
    "    \n",
    "    if main_content is None:\n",
    "        print(f\"No Wikipedia page found for {person_name}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Clean the main content\n",
    "    main_content = clean_text(main_content)\n",
    "    \n",
    "    # Collect and clean Wikipedia pages from the nearest neighbors\n",
    "    neighbor_contents = []\n",
    "    for neighbor in nearest_neighbors:\n",
    "        content = get_wikipedia_content(neighbor)\n",
    "        if content is not None:\n",
    "            neighbor_contents.append(clean_text(content))\n",
    "    \n",
    "    if len(neighbor_contents) < 2:\n",
    "        print(\"Not enough data to perform analysis. Skipping ranking.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Create Bag of Words and TF-IDF representations\n",
    "    bow_vectorizer = CountVectorizer(stop_words='english')\n",
    "    bow_matrix = bow_vectorizer.fit_transform([main_content] + neighbor_contents)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([main_content] + neighbor_contents)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    bow_similarity = cosine_similarity(bow_matrix[0].reshape(1, -1), bow_matrix[1:])\n",
    "    tfidf_similarity = cosine_similarity(tfidf_matrix[0].reshape(1, -1), tfidf_matrix[1:])\n",
    "    \n",
    "    # Calculate rankings\n",
    "    bow_ranking = np.argsort(bow_similarity[0])[::-1]\n",
    "    tfidf_ranking = np.argsort(tfidf_similarity[0])[::-1]\n",
    "    \n",
    "    # Create ranked lists of neighbors\n",
    "    bow_wikipedia_ranking = [nearest_neighbors[i] for i in bow_ranking]\n",
    "    tfidf_wikipedia_ranking = [nearest_neighbors[i] for i in tfidf_ranking]\n",
    "    \n",
    "    return main_content, bow_wikipedia_ranking, tfidf_wikipedia_ranking, tfidf_matrix\n",
    "\n",
    "# Test the function\n",
    "person_name = \"Albert Einstein\"  # Use a well-known person as an example\n",
    "nearest_neighbors = [\"Marie Curie\", \"Isaac Newton\", \"Galileo Galilei\", \"Stephen Hawking\", \"Richard Feynman\", \"Nikola Tesla\", \"Charles Darwin\", \"Aristotle\", \"Archimedes\", \"Leonardo da Vinci\"]\n",
    "\n",
    "main_content, bow_wikipedia_ranking, tfidf_wikipedia_ranking, tfidf_matrix = analyze_wikipedia_content(person_name, nearest_neighbors)\n",
    "\n",
    "if main_content is not None:\n",
    "    print(f\"\\nBoW Ranking:\")\n",
    "    for i, neighbor in enumerate(bow_wikipedia_ranking):\n",
    "        print(f\"{i+1}. {neighbor}\")\n",
    "\n",
    "    print(\"\\nTF-IDF Ranking:\")\n",
    "    for i, neighbor in enumerate(tfidf_wikipedia_ranking):\n",
    "        print(f\"{i+1}. {neighbor}\")\n",
    "\n",
    "    # Visualization\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_svd = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_svd[1:, 0], X_svd[1:, 1], alpha=0.5)\n",
    "    plt.scatter(X_svd[0, 0], X_svd[0, 1], color='red', s=100, label=person_name)\n",
    "    plt.title(f\"TF-IDF Visualization of {person_name} and Nearest Neighbors\")\n",
    "    plt.xlabel(\"SVD Dimension 1\")\n",
    "    plt.ylabel(\"SVD Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate sentiment of the entire page\n",
    "    main_sentiment = TextBlob(main_content).sentiment\n",
    "    print(f\"\\nSentiment of {person_name}'s Wikipedia page:\")\n",
    "    print(f\"Polarity: {main_sentiment.polarity}\")\n",
    "    print(f\"Subjectivity: {main_sentiment.subjectivity}\")\n",
    "\n",
    "    # Print out the Wikipedia article\n",
    "    print(f\"\\nWikipedia article for {person_name}:\")\n",
    "    print(main_content[:500] + \"...\")\n",
    "\n",
    "    # Compare rankings\n",
    "    original_ranking = nearest_neighbors\n",
    "    print(\"\\nComparison of rankings:\")\n",
    "    for i in range(len(original_ranking)):\n",
    "        if original_ranking[i] in tfidf_wikipedia_ranking:\n",
    "            print(f\"{original_ranking[i]}: Original rank {i+1}, Wikipedia rank {tfidf_wikipedia_ranking.index(original_ranking[i])+1}\")\n",
    "        else:\n",
    "            print(f\"{original_ranking[i]}: Original rank {i+1}, Not found in Wikipedia ranking\")\n",
    "else:\n",
    "    print(f\"No Wikipedia page found for {person_name}. Unable to perform analysis.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
