Recall that with a decision tree we got an RMSE of about \$3800, and with a random forest with got an RMSE of about \$2850.
#XGBoost
Here we are fitting an XGBoost model without specifying any hyperparameters.
## XGBoost with Parameters
Now we will see if we can improve performance by changing some parameters.
Now we will fit our XGBoost model with the best parameters
import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error

import xgboost as xgb

from sklearn.model_selection import GridSearchCV
# from google.colab import drive

# drive.mount('/content/drive', readonly=True)
url = "https://ddc-datascience.s3.amazonaws.com/boston.csv"

boston = pd.read_csv( url, index_col = 0 )

boston.head()
boston.info()
X = boston.drop('med_home_value', axis = 1)

y = boston['med_home_value']
numLoops = 100

mse_xgb  = np.zeros(numLoops)



for idx in range(0,numLoops):

  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15)

  xgbr = xgb.XGBRegressor(objective ='reg:squarederror', verbosity=0, seed = 10)

  xgbr.fit(X_train,y_train)

  y_pred_xgb = xgbr.predict(X_test)

  mse_xgb[idx] = mean_squared_error(y_test,y_pred_xgb)





print(f'RMSE: {np.sqrt(mse_xgb).mean()*1000}')
y_pred_xgb[0]
y_test.to_numpy()[0]
my_house = X_test[:3]

my_house
xgbr.predict(my_house)
# Specify the parameters you want to try and their ranges.

param_test = {

 'max_depth'     : [ 3, 4, 5, 6, 7 ],

 'learning_rate' : [ 0.1, 0.2, 0.3, 0.4 ],

 'n_estimators'  : [ 20, 40, 60, 80, 100, 120, 140 ],

}



# Perform the grid search

gsearch = GridSearchCV(

    estimator = xgb.XGBRegressor( objective = 'reg:squarederror', seed = 10 ),

    param_grid = param_test,

    scoring = 'neg_mean_squared_error',

    cv = 5,

)



# Fit to training data

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.05, random_state = 42 )

model = gsearch.fit(X_train,y_train)



# See grid search results

print(model.best_params_)
numLoops = 100

mse_xgb  = np.zeros(numLoops)



for idx in range(0,numLoops):

  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.15 )

  xgbr = xgb.XGBRegressor(

      objective ='reg:squarederror',

      verbosity=0,

      learning_rate = 0.2,

      max_depth = 3,

      n_estimators = 100,

      seed = 10,

  )

  xgbr.fit(X_train,y_train)

  y_pred_xgb = xgbr.predict(X_test)

  mse_xgb[idx] = mean_squared_error(y_test,y_pred_xgb)



print(f'RMSE: {np.sqrt(mse_xgb).mean()*1000}')
feat_imp = pd.Series(xgbr.feature_importances_, index=X.columns)

plt.figure(figsize = (10,6))

# ax = sns.barplot(x = feat_imp.index, y = feat_imp.values)

ax = sns.barplot(x = feat_imp.index, y = feat_imp.sort_values( ascending = False ).values )

ax.set_xticklabels( ax.get_xticklabels(), rotation = 90 )

plt.xlabel('Feature')

plt.ylabel('Feature Importance Score') ;
(feat_imp.sort_values( ascending = False )*100).cumsum()

